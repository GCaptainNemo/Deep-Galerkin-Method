{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络解偏微分方程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一. 介绍\n",
    "偏微分方程数值解法包括基于网格的有限差分、有限元、有限体等方法。这些方法需要使用网格来划分偏微分方程的定义域，网格越细得到的解越精确。越细的网格需要更高的计算代价与更大的存储空间。目前无网格主要是蒙特卡洛方法，使用Feynman-Kac公式将偏微分方程中的待求函数表示成随机过程中随机变量的数学期望，再通过蒙特卡洛求期望的办法求解偏微分方程。蒙特卡洛法的收敛性依赖于空间中采样的点的个数，采样的点越多，我们能得到越精确的解。但相应地，采样点越多，我们的计算时间越长和存储空间越大，在非常高维的情况下，仍然需要很大的计算量。\n",
    "\n",
    "最近，基于深度学习的方法求解偏微分方程得到了越来越多的关注。使用神经网络来表示偏微分方程中的待求函数，通过学习神经网络的参数来求得偏微分方程的近似解。相较于有网格的方法，深度学习方法求解参数的数量大大降低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、DGM方法\n",
    "神经网络就是一个函数族$\\{u_\\theta\\}$，DGM认为偏微分方程的解函数$f$可以用函数族中的某个函数近似$u_\\theta^*$。我们考虑如下形式的偏微分方程：\n",
    "\n",
    "$$\\begin{align}\n",
    "& \\frac{\\partial u(t, x) }{\\partial t} + F[u(t, x)] = 0 \\\\\n",
    "& u(0, x)  = u_0(x), \\ x\\in \\Omega \\\\\n",
    "& u(t, x) = g(t, x), \\ (t, x)\\in [0, T]\\times \\partial \\Omega\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要求解一个偏微分方程，除了要知道偏微分方程的形式之外，还需要知道初始条件$ \\ u_0(x)$和边界条件$ \\ u(t, x) x\\in \\partial \\Omega$。神经网络需要逼近方程的解$ \\ u^*(t, x)$，定义神经网络期望损失函数(expected loss function)如下\n",
    "\n",
    "$$\\begin{aligned}\n",
    "J(\\theta) = ||\\frac{\\partial u(t, x) }{\\partial t} + F[u(t, x)]||^{2}_{[0, T]\\times \\partial \\Omega,\n",
    "\\mu}  + \n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
